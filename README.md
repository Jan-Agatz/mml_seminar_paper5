## Introduction
This is my fork of _, which was created by _ and _. It was done as part of the seminar "" at Heidelberg University in the winter term 2023/2024. It provides code to reproduce Figure 1. in _, as well as experimental empirical data regarding non-constant stepsize schedules in SGD, i.e for non-constant stepsize schedules for the learning rate in the backpropagation part of the deep learning algorithm.
My fork adds several new non-constant, non-periodic stepsize schedules, as introduced by _, _ and _.

## Structure
'''_''' contains the different stepsize schedules. 
'''_''' contains the experimental setup for using the different stepsize schedules on least-square-problems, i.e. deterministic problems.
'''_''' contains the experimental setup for using the different stepsize schedules on MNIST, i.e. a stochastic problem.
